{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bot Adventure: The First Steps Building With GenAI\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Who?](#who)\n",
    "* [A Little Bit Of Context](#a-little-bit-of-context)\n",
    "* [Tools For Trade](#tools-for-trade)\n",
    "* [Let's Write Some Code](#lets-write-some-code)\n",
    "* [References](#references)\n",
    "* [Bonus](#bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Who?](#contents)\n",
    "\n",
    "Agnostic software engineer and technology enthusiast with high rates of caffeine. Full-time dad, (surf|skat)er and guitar player during free time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [A Little Bit Of Context](#contents)\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:rgb(22, 25, 34); border:1px solid #F8F8F2; color: #F8F8F2\">\n",
    "    <center>Artificial Intelligence</center>\n",
    "    <div class=\"alert\" style=\"background-color: rgb(37, 43, 63); border:1px solid #F8F8F2; color: #F8F8F2\">\n",
    "        <center>Machine Learning</center>\n",
    "        <div class=\"alert\" style=\"background-color:rgb(57, 66, 95); border:1px solid #F8F8F2; color: #F8F8F2\">\n",
    "            <center>Deep Learning</center>\n",
    "            <div class=\"alert\" style=\"background-color:rgb(69, 80, 114); border:1px solid #F8F8F2; color: #F8F8F2\">\n",
    "                <center>Generative AI</center>\n",
    "                <div class=\"alert\" style=\"background-color:rgb(83, 97, 139); border:1px solid #F8F8F2; color: #F8F8F2\">\n",
    "                <center>Large Language Models</center>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### Artificial Intelligence, AI\n",
    "\n",
    "Refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making ([Wikipedia](https://en.wikipedia.org/wiki/Artificial_intelligence)).\n",
    "\n",
    "### Machine Learning, ML\n",
    "\n",
    "Is a field of study in AI concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions ([Wikipedia](https://en.wikipedia.org/wiki/Machine_learning)).\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "A subset of ML that focuses on utilizing Artificial Neural Networks, ANNs, to perform tasks such as classification, regression, and representation learning  ([Wikipedia](https://en.wikipedia.org/wiki/Deep_learning)).\n",
    "\n",
    "<img src=\"./assets/ANN.png\" width=\"1000\">\n",
    "\n",
    "Image: [Total Product Marketing](https://totalproductmarketing.com/wp-content/uploads/2023/10/artificial-neural-networks-diagram.png).\n",
    "\n",
    "### Generative AI, GenAI\n",
    "\n",
    "A subset of AI that uses generative models to produce text, images, videos, or other forms of data ([Wikipedia](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)).\n",
    "\n",
    "### Large Language Models, LLMs\n",
    "\n",
    "Type of model designed for natural language processing, NLP, tasks such as language generation ([Wikipedia](https://en.wikipedia.org/wiki/Large_language_model)).\n",
    "\n",
    "LLMs are a particular type of ANN which applies *tokenization* and *embedding* to prepare the data, so the LLM can process and understand textual data.\n",
    "\n",
    "Tokenization breaks down text into smaller units and embeddings convert the tokens into numerical vectors.\n",
    "\n",
    "<img src=\"./assets/Tokenization.png\" width=\"1000\">\n",
    "\n",
    "Image: [Prismic](https://images.prismic.io/deepset/25e1c1fe-39de-4965-9fc3-b65159a90983_tobeornot.png).\n",
    "\n",
    "In 2017, the Transformer architecture improved and revolutionized the application of ANN.\n",
    "\n",
    "<img src=\"./assets/Transformers.png\" width=\"500\">\n",
    "\n",
    "Image: [Clipart Library](https://clipart-library.com/image_gallery2/Transformers-Logo-PNG-Image.png).\n",
    "\n",
    "#### Some Generative Foundation Models\n",
    "\n",
    "<img src=\"./assets/OpenAI.png\" width=\"100\"> <img src=\"./assets/Gemini.png\" width=\"100\"> <img src=\"./assets/Llama.png\" width=\"100\"> <img src=\"./assets/Claude.png\" width=\"100\"> <img src=\"./assets/DeepSeek.png\" width=\"100\">\n",
    "\n",
    "Image: [Lobe Hub](https://lobehub.com/icons).\n",
    "\n",
    "<img src=\"./assets/Done.jpg\">\n",
    "\n",
    "Image: [Quick Meme](http://www.quickmeme.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tools For Trade](#contents)\n",
    "\n",
    "### Local Setup\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Python >= 3.13\n",
    "* Pipenv >= 2024.0.1\n",
    "\n",
    "From a terminal, create a Python virtual environment with `pipenv`:\n",
    "\n",
    "`PIPENV_CUSTOM_VENV_NAME=\"Chat Bot\" pipenv shell`\n",
    "\n",
    "`pip install --upgrade pip`\n",
    "\n",
    "`pipenv install notebook`\n",
    "\n",
    "Then, run the notebook and point the kernel to the newly created environment:\n",
    "\n",
    "`jupyter notebook`\n",
    "\n",
    "### Online Setup\n",
    "\n",
    "* Go to the Try Jupyter page at [https://jupyter.org/try-jupyter](https://jupyter.org/try-jupyter)\n",
    "* Select Notebook Python (Pyodide)\n",
    "\n",
    "### Getting A Groq API Key\n",
    "\n",
    "* Go to the Groq Cloud console page at [https://console.groq.com/login](https://console.groq.com/login)\n",
    "* Create or access an existing account with e-mail, GitHub or Google account\n",
    "* After receive the accessing code by e-mail, go to [API Keys](https://console.groq.com/keys) page\n",
    "* Create a new API Key, giving a meaningful name\n",
    "* Copy the API Key value\n",
    "* Create a new file called `.env`, and add the API Key to the `API_KEY` variable: `API_KEY=\"MY_API_KEY\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Let's Write Some Code](#contents)\n",
    "\n",
    "GitHub repository: [https://github.com/furansa/chat-bot-adventure](https://github.com/furansa/chat-bot-adventure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel Chat Bot Assistant\n",
    "\n",
    "Context:\n",
    "\n",
    "We are going to build a chat bot to assist the guests of a famous hotel.\n",
    "\n",
    "The chat bot will provide information about reservations, promotions, policies, facilities, amenities, attractions, events, as well as handle feedbacks and complaints.\n",
    "\n",
    "The source of all information will be provided by the hotel direction.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. The chat bot must provide answers only based on the provided information\n",
    "2. Questions not related with the provided context must be ignoring by the chat bot\n",
    "3. The chat bot must be implemented using OpenAI API\n",
    "4. The chat bot implementation must be as much decoupled as possible, to favors future code reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting And Importing The Environment Variables\n",
    "\n",
    "Open and edit the existing `.env` file, then add:\n",
    "\n",
    "```\n",
    "API_KEY=\"MY_API_KEY\"\n",
    "API_URL=\"https://api.groq.com/openai/v1\"\n",
    "MODEL=\"mixtral-8x7b-32768\"\n",
    "```\n",
    "\n",
    "Import the environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an online notebook, it could be easier to define the environment variables on the notebook, so, uncomment the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"API_KEY\"]=\"MY_API_KEY\"\n",
    "# os.environ[\"API_URL\"]=\"https://api.groq.com/openai/v1\"\n",
    "# os.environ[\"MODEL\"]=\"mixtral-8x7b-32768\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"API_URL\"))\n",
    "print(os.getenv(\"MODEL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Information From Provided Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyMuPDF==1.25.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    try:\n",
    "        document = pymupdf.open(pdf_file)\n",
    "    except Exception as e:\n",
    "        return f\"Error opening PDF, {e}\"\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    for page in document:\n",
    "        text = page.get_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating The OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai==1.63.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_client() -> OpenAI:\n",
    "    return OpenAI(\n",
    "        api_key=os.getenv('API_KEY'),\n",
    "        base_url=os.getenv('API_URL')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating A Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(client: OpenAI, temperature: float, prompt_template: str, question: str) -> str:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt_template,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ],\n",
    "        model=os.getenv('MODEL'),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(\"./assets/Information.pdf\")\n",
    "\n",
    "prompt_template = text + \"\"\"\n",
    "You are the hotel manager of Landon Hotel, named \"Mr. Landon\".\n",
    "Your expertise is exclusively in providing information and advice about anything related to Landon Hotel.\n",
    "This includes any general Landon Hotel related queries.\n",
    "You do not provide information outside of this scope.\n",
    "If a question is not about Landon Hotel, respond with, \"I can't assist you with that, sorry!\"\n",
    "\"\"\"\n",
    "\n",
    "client = get_client()\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temperature)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting The Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"Who is Arthur Landon?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"What are the amenities for the guests?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"What is the check-in time?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"Can you suggest a japanese restaurant?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"And how can I register a complaint?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"Who are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"What is 42?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(client, temperature, prompt_template, \"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Configuring The Model To Explore A Little Bit Further](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating A New Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = \"\"\"\n",
    "You are a funny and creative assistant.\n",
    "You will try your best to assist the user with any questions they have.\n",
    "You will not provide any information which is offensive, harmful or not appropriate for children.\n",
    "\"\"\"\n",
    "\n",
    "new_client = get_client()\n",
    "new_temperature = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_temperature)\n",
    "print(new_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"Who are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"And who am I?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"What is 42?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"The cat is on the...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"And what else?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"Knock knock...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"Podemos falar em Português?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(new_client, new_temperature, new_prompt_template, \"Quem é Bruno Aleixo?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Talking To A Pirate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_prompt_template = \"\"\"\n",
    "You are a pirate assistant.\n",
    "You will always respond in pirate language.\n",
    "You will try your best to assist the user with any questions they have.\n",
    "You will not provide any information which is offensive, harmful or not appropriate for children.\n",
    "\"\"\"\n",
    "\n",
    "pirate_client = get_client()\n",
    "pirate_temperature = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pirate_temperature)\n",
    "print(pirate_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(pirate_client, pirate_temperature, pirate_prompt_template, \"Knock knock...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(pirate_client, pirate_temperature, pirate_prompt_template, \"The cat is on the...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(pirate_client, pirate_temperature, pirate_prompt_template, \"Tell me a pirate joke.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(pirate_client, pirate_temperature, pirate_prompt_template, \"Teach me some pirate food receipt.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Concluding](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(pirate_client, pirate_temperature, pirate_prompt_template, \"Say goodbye to the audience.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Thats All.jpg\" width=\"800\">\n",
    "\n",
    "Image: [Pinterest](https://i.pinimg.com/originals/01/57/37/015737689a6965be89e8ed49f5a3ba57.jpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [References](#contents)\n",
    "\n",
    "* [LLMs Glossary](https://www.coursera.org/collections/llms-terms)\n",
    "* [What is responsible AI?](https://www.ibm.com/think/topics/responsible-ai)\n",
    "* [Installing Jupyter](https://jupyter.org/install)\n",
    "* [Groq quickstart](https://console.groq.com/docs/quickstart)\n",
    "* [OpenAI Python API library](https://aifor.dev/02_understanding_large_language_models/02_00_understanding_large_language_models_introduction.html)\n",
    "* [LangChain tutorials](https://python.langchain.com/docs/tutorials)\n",
    "* [Getting started with Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-python)\n",
    "* [Ollama Linux installation](https://github.com/ollama/ollama/blob/main/docs/linux.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bonus](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using A Local Model Server\n",
    "\n",
    "Verify the server is up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/bin/ollama -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/bin/ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show information about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/bin/ollama show llama3.1:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/bin/ollama run llama3.1:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact With The Model Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-ollama==0.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.1:8b\", temperature=0.5)\n",
    "\n",
    "for chunk in model.stream(\"The first man on the moon was...\"):\n",
    "    print(chunk, end=\" \", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Are you sure?\"):\n",
    "    print(chunk, end=\" \", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the running model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/bin/ollama stop llama3.1:8b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chat Bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
